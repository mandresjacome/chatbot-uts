import { GoogleGenerativeAI } from '@google/generative-ai';
import { isTeacherSearchQuery, findTeacherByName, formatTeacherInfo } from '../nlp/teacherSearch.js';

// Funci√≥n para detectar consultas de malla curricular
function isMallaQuery(question) {
  const mallaKeywords = [
    'malla', 'pensum', 'plan de estudios', 'curriculum', 'materias',
    'asignaturas', 'semestres', 'niveles', 'cr√©ditos', 'estructura curricular'
  ];
  
  const questionLower = question.toLowerCase();
  return mallaKeywords.some(keyword => questionLower.includes(keyword));
}

// Funci√≥n para generar respuesta con componente de malla curricular
function generateMallaResponse(question) {
  const questionLower = question.toLowerCase();
  
  // Determinar si se pregunta por un programa espec√≠fico
  let programa = null;
  if (questionLower.includes('tecnolog√≠a') || questionLower.includes('tecnolog')) {
    programa = 'tecnologia';
  } else if (questionLower.includes('ingenier√≠a') || questionLower.includes('ingenier')) {
    programa = 'ingenieria';
  }

  return `
üéì **Malla Curricular - Unidades Tecnol√≥gicas de Santander**

üìã Te muestro la estructura curricular de nuestros programas de Sistemas. Puedes navegar por los diferentes niveles acad√©micos y explorar las materias de cada semestre.

${programa ? `üîç Has consultado espec√≠ficamente sobre **${programa === 'tecnologia' ? 'Tecnolog√≠a en Desarrollo de Sistemas Inform√°ticos' : 'Ingenier√≠a de Sistemas'}**.` : 'üîç Puedes explorar tanto **Tecnolog√≠a** como **Ingenier√≠a de Sistemas**.'}

**MALLA_CURRICULAR_COMPONENT**

üí° **Caracter√≠sticas principales:**
üìå Navega por niveles usando las flechas
üìå Haz clic en cualquier materia para ver detalles
üìå Usa el zoom para mejor visualizaci√≥n
üìå Cambia entre programas con los botones superiores

¬øTe gustar√≠a informaci√≥n espec√≠fica sobre alguna materia o nivel en particular? ü§ì
`.trim();
}

const USE_LLM = process.env.USE_LLM || 'gemini';
const MODEL = process.env.GEMINI_MODEL || 'gemini-2.5-flash';
const hasKey = Boolean(process.env.GEMINI_API_KEY);

const MAX_EVIDENCE = Number(process.env.MAX_CHARS_EVIDENCE || 2500);
const MAX_RESPONSE = Number(process.env.MAX_CHARS_RESPONSE || 1800);

// Recorta texto por caracteres (defensivo)
function cut(text = '', max = 2000) {
  if (!text) return '';
  return text.length > max ? text.slice(0, max) + '‚Ä¶' : text;
}

// Prompt "evidencia primero" con contexto de conversaci√≥n
function buildPrompt({ question, evidenceChunks, userType, conversationHistory = [] }) {
  const system = [
    'Eres el Chatbot UTS v1.2.0 para las Unidades Tecnol√≥gicas de Santander.',
    'Responde √öNICAMENTE con la evidencia proporcionada.',
    'Si la evidencia no basta, pide datos concretos (programa, sede, periodo).',
    `Perfil del usuario: ${userType}.`,
    'Tono institucional, claro y conciso. Formatea con vi√±etas si ayuda.',
    'USA EMOJIS relevantes para hacer las respuestas m√°s visuales y atractivas.',
    'Incluye emojis al inicio de secciones importantes y en las vi√±etas.',
    'NO incluyas referencias ni menciones de fuentes en tu respuesta.',
    'MANT√âN COHERENCIA con la conversaci√≥n anterior si existe contexto previo.',
  ].join(' ');

  const evidence = evidenceChunks.length
    ? evidenceChunks.map((c,i)=> `[#${i+1}] ${c.text}`).join('\n')
    : '(no hay evidencia disponible)';

  // Incluir historial de conversaci√≥n si existe
  let contextSection = '';
  if (conversationHistory.length > 0) {
    const historyText = conversationHistory.map(conv => 
      `Usuario: ${conv.pregunta}\nAsistente: ${conv.respuesta}`
    ).join('\n\n');
    contextSection = `CONVERSACI√ìN ANTERIOR:\n${cut(historyText, 800)}\n\n`;
  }

  const body = [
    system,
    '',
    contextSection,
    `Pregunta actual del usuario: ${question}`,
    '',
    'EVIDENCIA:',
    cut(evidence, MAX_EVIDENCE),
    '',
    `Redacta la mejor respuesta posible en <= ${MAX_RESPONSE} caracteres, considerando el contexto de la conversaci√≥n y siendo fiel a la evidencia.`
  ].join('\n');

  return body;
}

// --- Cliente Gemini (si hay API key) ---
let genAI = null;
if (hasKey) {
  genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
}

export async function answerLLM({ question, evidenceChunks, userType, conversationHistory = [] }) {
  // Verificar si es consulta sobre malla curricular
  if (isMallaQuery(question)) {
    return generateMallaResponse(question);
  }

  // Verificar si es una b√∫squeda de docente espec√≠fico
  if (isTeacherSearchQuery(question)) {
    // Buscar informaci√≥n de docentes en los chunks de evidencia
    const teacherChunk = evidenceChunks.find(chunk => 
      chunk.text.includes('DOCENTE') || 
      chunk.text.includes('Correo Institucional') ||
      chunk.text.toLowerCase().includes('docente') && chunk.text.includes('@uts.edu.co')
    );
    
    if (teacherChunk) {
      const teacher = findTeacherByName(question, teacherChunk.text);
      if (teacher) {
        return `¬°Informaci√≥n encontrada! üéì\n\n${formatTeacherInfo(teacher)}\n\nüìç **Programa:** Ingenier√≠a de Sistemas - UTS\n\n¬øTe gustar√≠a conocer algo m√°s espec√≠fico sobre este docente o el programa?`;
      }
    }
  }

  // Fallback mock o sin clave con contexto
  if (USE_LLM === 'mock' || !hasKey) {
    let contextNote = '';
    if (conversationHistory.length > 0) {
      contextNote = '\n\nüí¨ Continuando nuestra conversaci√≥n anterior... ';
    }
    
    if (!evidenceChunks?.length) {
      return `ü§î No tengo evidencia suficiente sobre "${question}". ` +
             `üìù Ind√≠came programa/sede/periodo para ayudarte mejor.${contextNote}`;
    }
    const bullets = evidenceChunks.map((c,i)=> `üìå ${c.text}`).join('\n');
    return `${bullets}${contextNote}\n\n‚ùì ¬øDeseas detalles para tu programa o sede espec√≠ficos?`;
  }

  // Gemini "evidencia primero" con historial de conversaci√≥n
  const model = genAI.getGenerativeModel({ model: MODEL });

  const prompt = buildPrompt({ question, evidenceChunks, userType, conversationHistory });
  const result = await model.generateContent({
    contents: [{ role: 'user', parts: [{ text: prompt }] }]
  });

  // Cortamos por seguridad/estilo
  const out = result?.response?.text?.() || '';
  return cut(out, MAX_RESPONSE);
}
